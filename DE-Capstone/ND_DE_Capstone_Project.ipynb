{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Relationship between several factors and US immigration events\n",
    "\n",
    "## Project Summary\n",
    "Project to get analytic table to show relationship between several factors and US immigration events\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DateType, IntegerType, DoubleType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.\\\n",
    "            config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "            .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project I extracted data from different sources, transformed them with Spark operations and loaded them into several dimensionsional tables. \n",
    "The fact table is about the effect of several factors on the immigration movements in the U.S. in 2016.  \n",
    "\n",
    "#### Describe and Gather Data\n",
    "1. **I94 Immigration Data**: This data comes from the US National Tourism and Trade Office.\n",
    "2. **I94_SAS_Labels_Description**: Description of data attributes of I94 Immigration data\n",
    "2. **US-Cities-Demographics**: U.S. City Demographic Data from OpenSoft\n",
    "3. **World Temperature Data**: This dataset came from Kaggle\n",
    "4. **Airport Code Table**: This is a simple table of airport codes and corresponding cities. It comes from datahub.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "im_data = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "sas_labels = 'I94_SAS_Labels_Descriptions.SAS'\n",
    "demogr_data = \"us-cities-demographics.csv\"\n",
    "temp_data = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "airport_codes = 'airport-codes_csv.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "In all datasets we have issues with NaNs and scope. Since we are checking the effect of several factors on the immigration numbers in 2016 we can already scope the data based on that year. \n",
    "\n",
    "\n",
    "#### Cleaning Steps\n",
    "\n",
    "1. **I94 Immigration Data**: \n",
    "    - In this example notebook I only selected one month (april-2016), but can be scaled up to all data.\n",
    "    - Filtered on valid states in the **I94_SAS_Labels_Description** file\n",
    "2. **I94_SAS_Labels_Description**: \n",
    "    - we can get all valid cities and airports in the immigrants dataset, which will be the main datasource for our analytical table. This is a raw text format so I extracted the nessesary fiels with regex functions to extract the valid ports, the valid cities, the visa codes, the visa modes and countries.\n",
    "2. **US-Cities-Demographics**:\n",
    "    - The trick with this dataset is to pivot on state for the column Race (so you get the racial groups in the columns), State in the index and the counts as values \n",
    "3. **World Temperature Data**: \n",
    "    - Only selected valid cities from U.S. because of the scope. \n",
    "    - Past 3 years. \n",
    "4. **Airport Code Table**:\n",
    "    - In this table there the column iata_code respresents the code for the airport. The make it feasible for this analysis I dropped the NaNs\n",
    "    - Airports with `type=='closed'`are filtered out. \n",
    "    - Drop duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "![Data-Model](data-model-capstone.PNG \"Data Model for Immigration data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "- **DIM_airports**: Get the valid ports from **I94_SAS_Labels_Description** and join them with the valid fields in **Airport Code Table**\n",
    "- **DIM_visa**: Get the reason for immigration from **I94_SAS_Labels_Description**\n",
    "- **DIM_mode**: Get the travel mode from **I94_SAS_Labels_Description**\n",
    "- **DIM_state**: \n",
    "    - Get the demographic data per state from **US-Cities-Demographics**\n",
    "    - Get the valid cities from **I94_SAS_Labels_Description**\n",
    "    - Get the temperature data per valid city from **World Temperature Data**\n",
    "    - Aggregate and pivot the data per state\n",
    "- **DIM_country**: Get the country id and names from **I94_SAS_Labels_Description**\n",
    "- **FACT_immigration**: \n",
    "    - Get the valid ports from **I94_SAS_Labels_Description**\n",
    "    - Cast double types to integer so we can later join the id's with the dimensions and sas-date fields to datetime format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DIM_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+\n",
      "|mode_id|        mode|\n",
      "+-------+------------+\n",
      "|      1|         Air|\n",
      "|      3|        Land|\n",
      "|      9|Not reported|\n",
      "|      2|         Sea|\n",
      "+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flist = open('I94_SAS_Labels_Descriptions.SAS').readlines()\n",
    "idx = [k for (k,v) in enumerate(flist) if v.startswith(\"/* I94ADDR\") or v.startswith(\"value i94model\")]\n",
    "p = re.compile(\"\\t(\\d) = '(.*)'\")\n",
    "dim_mode = spark.createDataFrame(p.findall(''.join(flist[idx[0]:idx[1]][1:])), schema=['mode_id', 'mode'])\n",
    "dim_mode = dim_mode.withColumn(\"mode_id\", F.col(\"mode_id\").cast(\"integer\")).dropDuplicates(['mode_id'])\n",
    "dim_mode.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DIM_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|visa_id|    visa|\n",
      "+-------+--------+\n",
      "|      1|Business|\n",
      "|      3| Student|\n",
      "|      2|Pleasure|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = [k for (k,v) in enumerate(flist) if v.startswith(\"/* I94VISA\") or v.startswith(\"/* COUNT\")]\n",
    "p = re.compile(\"(\\d) = (\\w+)\\n\")\n",
    "dim_visa = spark.createDataFrame(p.findall(''.join(flist[idx[0]:idx[1]][1:])), schema=['visa_id', 'visa'])\n",
    "dim_visa = dim_visa.withColumn(\"visa_id\", F.col(\"visa_id\").cast(\"integer\")).dropDuplicates(['visa_id'])\n",
    "dim_visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DIM_airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-------------------------------------+--------------+---------+-----------+----------+-------------+\n",
      "|port_id|airport       |name                                 |type          |continent|iso_country|iso_region|municipality |\n",
      "+-------+--------------+-------------------------------------+--------------+---------+-----------+----------+-------------+\n",
      "|BGM    |BANGOR, ME    |Greater Binghamton/Edwin A Link field|medium_airport|NA       |US         |US-NY     |Binghamton   |\n",
      "|FMY    |FORT MYERS, FL|Page Field                           |medium_airport|NA       |US         |US-FL     |Fort Myers   |\n",
      "|LEB    |LEBANON, NH   |Lebanon Municipal Airport            |medium_airport|NA       |US         |US-NH     |Lebanon      |\n",
      "|DNS    |DUNSEITH, ND  |Denison Municipal Airport            |small_airport |NA       |US         |US-IA     |Denison      |\n",
      "|EGL    |EAGLE, AK     |Negele Airport                       |small_airport |AF       |ET         |ET-OR     |Negele Borana|\n",
      "+-------+--------------+-------------------------------------+--------------+---------+-----------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = [k for (k,v) in enumerate(flist) if v.startswith(\"/* I94PORT\") or v.startswith(\"/* ARRDATE\")]\n",
    "p = re.compile(\"'(.*)'\\t=\\t'(.*)'\")\n",
    "airport_list = [(i[0], i[1].rstrip()) for i in p.findall(''.join(flist[idx[0]:idx[1]][1:]))]\n",
    "df_valid_ports = spark.createDataFrame([(i[0], i[1].rstrip()) for i in p.findall(''.join(flist[idx[0]:idx[1]][1:]))], schema=['port_id', 'airport'])\n",
    "\n",
    "df_airport_codes = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ',')\\\n",
    "            .load(airport_codes)\n",
    "\n",
    "df_airport_codes = df_airport_codes\\\n",
    "                        .filter(F.col('iata_code').isNotNull())\\\n",
    "                        .filter(F.col('type')!='closed')\\\n",
    "                        .select('name', 'type', 'continent', 'iso_country', 'iso_region', 'municipality', 'iata_code')\\\n",
    "                        .withColumnRenamed('iata_code', 'port_id')\n",
    "\n",
    "dim_airports = df_valid_ports.join(df_airport_codes, on='port_id').dropDuplicates(['port_id'])\n",
    "dim_airports.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DIM_Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------------------------+\n",
      "|country_id|country                           |\n",
      "+----------+----------------------------------+\n",
      "|471       |INVALID: MARIANA ISLANDS, NORTHERN|\n",
      "|243       |BURMA                             |\n",
      "|392       |MALI                              |\n",
      "|737       |INVALID: MIDWAY ISLANDS           |\n",
      "|516       |TRINIDAD AND TOBAGO               |\n",
      "+----------+----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = [k for (k,v) in enumerate(flist) if v.startswith(\"/* I94CIT & I94RES\") or v.startswith(\"/* I94PORT\")]\n",
    "p = re.compile(\"(\\d{3}) = *'(.*)'\")\n",
    "dim_country = spark.createDataFrame(p.findall(''.join(flist[idx[0]:idx[1]][1:])), schema=['country_id', 'country'])\n",
    "dim_country = dim_country.withColumn(\"country_id\", F.col(\"country_id\").cast(\"integer\")).dropDuplicates(['country_id'])\n",
    "dim_country.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### DIM_state\n",
    "\n",
    "This dimension combines the data from the demographics and the temperature data. For The immigrant and temperature data we need to filter the cities and ports to only U.S. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "valid_ports_cities = [(i[0], i[1].split(', ')[0].title()) for i in airport_list  if len(i[1].split(', ')[-1])==2]\n",
    "valid_ports_cities = spark.createDataFrame(valid_ports_cities, schema=['port_id', 'city'])\n",
    "valid_ports = [i[0] for i in valid_ports_cities.select('port_id').distinct().collect()]\n",
    "valid_cities = [i[0] for i in valid_ports_cities.select('city').distinct().collect()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+----------------+------------+------------+-----------+----------+---------+-------+-------+---------+--------+\n",
      "|state_id|         state|total_population|num_veterans|foreign_born|avg_hh_size|num_native|num_asian|num_afr|num_lat|num_white|avg_temp|\n",
      "+--------+--------------+----------------+------------+------------+-----------+----------+---------+-------+-------+---------+--------+\n",
      "|      AZ|       Arizona|         4499542|      264505|      682313|       2.77|    129708|   229183| 296222|1508157|  3591611|   20.47|\n",
      "|      SC|South Carolina|          533657|       33463|       27744|       2.47|      3705|    13355| 175064|  29863|   343764|   19.74|\n",
      "|      LA|     Louisiana|         1300595|       69771|       83419|       2.47|      8263|    38739| 602377|  87133|   654578|   21.28|\n",
      "|      MN|     Minnesota|         1422403|       64894|      215873|        2.5|     25242|   151544| 216731| 103229|  1050239|    9.81|\n",
      "|      NJ|    New Jersey|         1428908|       30195|      477028|       2.97|     11350|   116844| 452202| 600437|   615083|   11.66|\n",
      "+--------+--------------+----------------+------------+------------+-----------+----------+---------+-------+-------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ';')\\\n",
    "            .load(demogr_data)\n",
    "df_demo = df_demo.withColumn(\"Count\", df_demo[\"Count\"].cast(IntegerType()))\n",
    "\n",
    "df_temp = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", ',')\\\n",
    "            .load(temp_data)\n",
    "\n",
    "def clean_temp_data(df, from_year, valid_cities):\n",
    "    \n",
    "    df = df\\\n",
    "            .withColumn(\"dt\", df_temp['dt'].cast(DateType()))\\\n",
    "            .filter(F.col('AverageTemperature').isNotNull())\\\n",
    "            .filter(F.col(\"City\").isin(valid_cities))\\\n",
    "            .filter(F.col(\"dt\")>from_year)\n",
    "                    \n",
    "    return df\n",
    "\n",
    "df_temp_clean = clean_temp_data(df_temp, from_year='2010', valid_cities = valid_cities)\n",
    "df_temp_clean = df_temp_clean.withColumn(\"AverageTemperature\", df_temp_clean[\"AverageTemperature\"].cast(DoubleType()))\n",
    "\n",
    "temp_per_state = df_demo.select('City', 'State Code').distinct()\\\n",
    "        .join(\n",
    "            df_temp_clean.withColumn(\"AverageTemperature\", df_temp_clean[\"AverageTemperature\"].cast(DoubleType()))\\\n",
    "                .groupby('City').agg(F.round(F.avg(\"AverageTemperature\"),2).alias(\"avg_temp\")), on='City')\\\n",
    "                .groupby('State Code').agg(F.round(F.avg('avg_temp'), 2).alias('avg_temp'))\n",
    "\n",
    "dim_state = df_demo.dropDuplicates(['State Code', 'City']).groupBy(\"State Code\", \"State\")\\\n",
    "    .agg(\n",
    "      F.sum(\"Total Population\").cast('integer').alias(\"total_population\"),\n",
    "      F.sum(\"Number of Veterans\").cast('integer').alias(\"num_veterans\"),\n",
    "      F.sum(\"Foreign-born\").cast('integer').alias(\"foreign_born\"),\n",
    "      F.round(F.avg(\"Average Household Size\"),2).alias(\"avg_hh_size\"))\\\n",
    "    .join(df_demo.groupBy(\"State Code\")\\\n",
    "        .pivot('Race').sum(\"Count\")\\\n",
    "        .select(F.col('State Code'),\n",
    "                F.col('American Indian and Alaska Native').alias('num_native'),\n",
    "                F.col('Asian').alias('num_asian'),\n",
    "                F.col('Black or African-American').alias('num_afr'),\n",
    "                F.col('Hispanic or Latino').alias('num_lat'),\n",
    "                F.col('White').alias('num_white')), on='State Code')\\\n",
    "    .join(temp_per_state, on='State Code')\\\n",
    "    .withColumnRenamed(\"State Code\",\"state_id\")\\\n",
    "    .withColumnRenamed(\"State\",\"state\").dropDuplicates(['state_id'])\n",
    "dim_state.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fact Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+--------+-------+--------------+--------------+-----+------+------+------+----------+----------+\n",
      "|im_id|port_id|visa_id|state_id|mode_id|country_id_cit|country_id_res|im_yr|im_mon|bir_yr|gender|   depdate|   arrdate|\n",
      "+-----+-------+-------+--------+-------+--------------+--------------+-----+------+------+------+----------+----------+\n",
      "|  148|    NEW|      2|      NY|      1|           103|           103| 2016|     4|    21|     F|2016-04-08|2016-04-01|\n",
      "|  463|    MIA|      2|      FL|      1|           103|           103| 2016|     4|    25|  null|2016-04-02|2016-04-01|\n",
      "|  471|    MIA|      2|    null|      2|           103|           103| 2016|     4|    63|     M|2016-04-03|2016-04-01|\n",
      "|  496|    CHI|      1|      IL|      1|           103|           103| 2016|     4|    64|  null|2016-04-04|2016-04-01|\n",
      "|  833|    BOS|      2|      NY|      1|           104|           104| 2016|     4|    16|     F|2016-04-09|2016-04-01|\n",
      "+-----+-------+-------+--------+-------+--------------+--------------+-----+------+------+------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_im = spark.read.format('com.github.saurfang.sas.spark')\\\n",
    "            .load(im_data)\n",
    "\n",
    "# Selecting the right columns from the Data Model\n",
    "col_sel = ['cicid', 'i94port', 'i94visa', 'i94addr', 'i94mode', 'i94cit', 'i94res', 'i94yr', 'i94mon', 'i94bir', 'gender', 'depdate', 'arrdate']\n",
    "new_col = ['im_id', 'port_id', 'visa_id', 'state_id', 'mode_id', 'country_id_cit', 'country_id_res', 'im_yr', 'im_mon', 'bir_yr', 'gender', 'depdate', 'arrdate']\n",
    "fact_im = fact_im.select([F.col(c).alias(dict(zip(col_sel, new_col)).get(c, c)) for c in col_sel])\n",
    "\n",
    "# Filter only valid ports\n",
    "fact_im = fact_im.filter(F.col('port_id').isin(valid_ports))\n",
    "\n",
    "# Casting \n",
    "\n",
    "## the double columns to integer type\n",
    "for col_name in [i[0] for i in fact_im.dtypes if i[1] =='double']:\n",
    "    fact_im = fact_im.withColumn(col_name, F.col(col_name).cast('integer'))\n",
    "\n",
    "## The two date columns to readable format\n",
    "def get_datetime(sas_days):\n",
    "    if sas_days:\n",
    "        return datetime(1960,1,1) + timedelta(days=sas_days)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "f_get_datetime = F.udf(get_datetime, DateType())\n",
    "\n",
    "fact_im = fact_im.withColumn('depdate', f_get_datetime('depdate'))\\\n",
    "        .withColumn('arrdate', f_get_datetime('arrdate')).dropDuplicates(['im_id'])\n",
    "\n",
    "fact_im.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "- All dimensional tables ends with a drop duplicates so we ensure we don't have duplicate keys in these tables. \n",
    "- All dimensinal tables has at least one row of data. See the test below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_zero_records(spark_df):\n",
    "    \"\"\"\n",
    "    Checks if input DataFrame has 0 records\n",
    "    \n",
    "    :param df: Spark DataFrame\n",
    "    \"\"\"\n",
    "    \n",
    "    check_zero = f\"In this DataFrame there are {spark_df.count()} records found\"\n",
    "    \n",
    "    if spark_df.count() == 0:\n",
    "        print(f\"Check failed: {check_zero}\")\n",
    "    else:\n",
    "        print(f\"Check passed: {check_zero}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: DIM_air:\n",
      "Check passed: In this DataFrame there are 537 records found\n",
      "\n",
      "\n",
      "Table: DIM_visa:\n",
      "Check passed: In this DataFrame there are 3 records found\n",
      "\n",
      "\n",
      "Table: DIM_mode:\n",
      "Check passed: In this DataFrame there are 4 records found\n",
      "\n",
      "\n",
      "Table: DIM_state:\n",
      "Check passed: In this DataFrame there are 40 records found\n",
      "\n",
      "\n",
      "Table: DIM_country:\n",
      "Check passed: In this DataFrame there are 287 records found\n",
      "\n",
      "\n",
      "Table: FACT_Immigration:\n",
      "Check passed: In this DataFrame there are 2873793 records found\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for table, spark_df in zip(['DIM_air', 'DIM_visa', 'DIM_mode', 'DIM_state', 'DIM_country', 'FACT_Immigration'], [dim_airports, dim_visa, dim_mode, dim_state, dim_country, fact_im]):\n",
    "    print(f'Table: {table}:')\n",
    "    check_zero_records(spark_df)\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "- **DIM_airports**:\n",
    "    - port_id: Identifier for airport (VARCHAR)\n",
    "    - airport: Name of the airport in format 'City, State' (VARCHAR)\n",
    "    - name: Official name of the airport (VARCHAR)\n",
    "    - type: Type of airport (VARCHAR)\n",
    "    - iso_country: Country of airport in iso-format (VARCHAR)\n",
    "    - iso_region: Region of airport in iso-format (VARCHAR)\n",
    "    - municipality: Municipality of the airport (VARCHAR)\n",
    "  \n",
    "  \n",
    "- **DIM_visa**:\n",
    "    - visa_id: Identifier for visa (INT)\n",
    "    - visa: Reason for immigration (VARCHAR)\n",
    "    \n",
    "    \n",
    "- **DIM_mode**:\n",
    "    - mode_id: Identifier for mode (INT)\n",
    "    - mode: Mode of travel (VARCHAR)\n",
    "\n",
    "\n",
    "- **DIM_state**:\n",
    "    - state_id: Identifier of U.S. state (VARCHAR)\n",
    "    - state: Name of the state (VARCHAR)\n",
    "    - total_population: Total populatin in state (INT)\n",
    "    - num_veraterans: Number of vetarans in state (INT)\n",
    "    - foreign_born: Number of foreign born people in state (INT)\n",
    "    - avg_hh_size: Average household size in state (DOUBLE)\n",
    "    - num_native: Number of American Indian and Alaska Native people in state (INT)\n",
    "    - num_asian: Number of Asian people in state (INT)\n",
    "    - num_afr: Number of Black or African-American people in state (INT)\n",
    "    - num_lat: Number of Hispanic or Latino people in state (INT)\n",
    "    - num_white: Number of White people in state (INT)\n",
    "    - avg_temp: Average temperature in state (DOUBLE)\n",
    "\n",
    "\n",
    "- **DIM_country**: \n",
    "    - country_id: Identifier of country (INT)\n",
    "    - country: Country (VARCHAR)\n",
    "    \n",
    "    \n",
    "- **FACT_immigration**: \n",
    "    - im_id: Identifier for immigration (INT)\n",
    "    - port_id: Identifier for airport (VARCHAR)\n",
    "    - visa_id: Identifier for visa (INT)\n",
    "    - mode_id: Identifier for mode (INT)\n",
    "    - state_id: Identifier of U.S. state (VARCHAR)\n",
    "    - country_id_cit: Identifier of country (INT)\n",
    "    - country_id_res: Identifier of country (INT)\n",
    "    - im_year: Year of immigration (INT)\n",
    "    - im_month: Month of immigration (INT)\n",
    "    - biryear: Year of birth of immigrant (INT)\n",
    "    - gender: Gender of immigrant (VARCHAR)\n",
    "    - depdate: Departure date (DATETIME)\n",
    "    - arrdate: Departure date (DATETIME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Write the tables to parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the dimensions\n",
    "for table, spark_df in zip(['DIM_air', 'DIM_visa', 'DIM_mode',  'DIM_state', 'DIM_country'], [dim_airports, dim_visa, dim_state, dim_country]):\n",
    "    spark_df.write.parquet(f'output_data/{table}/', mode='overwrite')\n",
    "    \n",
    "# Partion the fact by year and month\n",
    "fact_im.write.partitionBy('im_yr', 'im_mon').parquet(f'output_data/FACT_immigration/', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "\n",
    "1. If the data of this project would be increased by 100x, I would make use of distributed power. All scripts in this notebook are mode with PySpark so can be reused for scaling. \n",
    "2. In the notebook I followed the pipeline for one month of immigration data. If we can query this data frequently (for instance daily) and use it in a dashboard I would recoomend to use Airflow. In this workflow the different source data can be extracted, transformed and loaded into dimensional tables in for instance Amazon Redshift. With the start and end date parameters in Airflow we can choose to load a subset of the immigration data to partion on time. \n",
    "3. If the data needs to be accesse dby +100 people I would write the output to Amazon Redshift since it is optimized for doing aggregations and has a good read performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
